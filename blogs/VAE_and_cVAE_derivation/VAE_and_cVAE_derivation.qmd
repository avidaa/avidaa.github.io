# Mathematical Derivation of VAE

### What is the aim of blog?

This blog draws upon insights from [this article](https://hunterheidenreich.com/posts/modern-variational-autoencoder-in-pytorch/) and [this post](https://lilianweng.github.io/posts/2018-08-12-vae/#td-vae), with the objective of developing a deeper and more technical understanding of Variational Autoencoders (VAEs).

### Introduction: What are Variational Autoencoders (VAEs)?

Variational Autoencoders (VAEs) are a foundational class of latent variable models that combine probabilistic modeling with neural networks, enabling **both representation learning and data generation**. Unlike standard autoencoders, VAEs provide a principled probabilistic framework that allows us to reason about uncertainty, impose structure on latent spaces, and perform meaningful sampling.

At a high level, VAEs assume that high-dimensional observations are generated from a lower-dimensional latent variable through a stochastic process. Learning such models, however, presents a central challenge: *the true posterior distribution over latent variables is intractable*. Variational inference resolves this by introducing a tractable approximation and reframing learning as an optimization problem.

The goal of this post is to develop a precise and fully mathematical understanding of how VAEs are derived—from their generative assumptions to the Evidence Lower Bound (ELBO) that is optimized in practice. Rather than focusing on intuition alone, we walk through each step of the derivation, clarifying where approximations are introduced and why the final objective is both computable and effective.

### Intuition: What a VAE Is Really Learning

A VAE assumes that each data point can be explained by a small set of hidden factors—latent variables—that capture the essential structure of the data. 

The encoder does not map an input to a single point in latent space, but instead learns **a distribution over plausible latent representations**, reflecting uncertainty about how the data was generated. 

The decoder then learns how to **probabilistically reconstruct the data from samples drawn from this latent distribution**.

Training a VAE is therefore a balancing act: we want latent representations that are expressive enough to reconstruct the data well, while also being regularized to follow a simple prior distribution so that the latent space remains smooth and generative.

### What are the assumptions of a VAE model?

Assume we have a dataset $X$:

$$X = [\vec{x}^{(i)}]_{i=1}^N = \{\vec{x}^{(1)}, \vec{x}^{(2)}, \ldots, \vec{x}^{(N)}\}$$

Each $\vec{x}^{(i)}$ is IID. It can be continuous or discrete-valued.

**Key Generative Assumptions:**

The VAE framework makes the following fundamental assumptions about how the observed data is generated:

::: {.callout-note icon=false}
## Assumption 1: Latent Prior Distribution

Each latent vector $\vec{z}^{(i)}$ is drawn from a **prior distribution**:

$$\vec{z}^{(i)} \sim \color{purple}{p_{\theta^*}(\vec{z})}$$

where $\color{purple}{p_{\theta^*}(\vec{z})}$ is the $\color{purple}{\text{prior}}$ over the lower-dimensional latent space, parameterized by $\color{purple}{\theta^*}$.
:::

::: {.callout-note icon=false}
## Assumption 2: Conditional Likelihood

Each observed data point is generated from its corresponding latent vector through a **conditional distribution**:

$$\vec{x}^{(i)} \sim \color{blue}{p_{\theta^*}(\vec{x} \mid \vec{z} = \vec{z}^{(i)})}$$

where $\color{blue}{p_{\theta^*}(\vec{x} \mid \vec{z})}$ is the model's $\color{blue}{\text{likelihood}}$ function, representing the probability of generating $\vec{x}$ given latent code $\vec{z}$.
:::

In essence, we assume the observed high-dimensional dataset $X$ is generated by a **latent variable model** with an underlying lower-dimensional random process $\vec{z}$.



The goal is to find the parameter $\color{purple}{\theta^*}$ that makes our observed data as likely as possible under the model. In other words, we want to maximize the likelihood of the data:

$$\color{purple}{\theta^*} {\color{black}{= \arg \max_{\theta}\prod_{i=1}^N}} \color{teal}{p_\theta(\vec{x}^{(i)})}$$

However, it is usually more convenient and numerically stable to work with the log-likelihood (since logs turn products into sums). Therefore, we rewrite the objective as:

$$\color{purple}{\theta^*} \color{black}{ = \arg \max_{\theta}\sum_{i=1}^N \log} \; \color{teal}{p_\theta(\vec{x}^{(i)})}$$

To compute $\color{teal}{p_\theta(\vec{x}^{i})}$, we marginalize over the latent variable $z$:

$$\color{teal}{p_\theta(\vec{x}^{i})} = \int \color{blue}{p_\theta(x|z)}\, \color{orange}{p_\theta(z)}\, dz$$

However, directly computing this integral is typically **intractable** because it requires evaluating $p_\theta(x|z)$ for all possible values of $z$. To make this computation practical, we introduce an auxiliary function, $\color{green}{q_\phi(z|x)}$, called the variational distribution or approximate posterior. This function, parameterized by $\color{green}{\phi}$, provides a tractable way to estimate which values of $z$ are likely given a particular input $x$.

### What is the role of VAE's encoder and decoder?

To understand the VAE architecture, let's start with **Bayes' theorem**, which relates our key distributions:

$$\color{red}{p_\theta(z|x)} = \frac{\color{orange}{p_\theta(z)} \times \color{blue}{p_\theta(x|z)}}{\color{teal}{p_\theta(x)}}$$

This equation shows that the **posterior** distribution $\color{red}{p_\theta(z|x)}$ (the probability of latent code $z$ given observation $x$) can be computed from the prior $\color{orange}{p_\theta(z)}$, the likelihood $\color{blue}{p_\theta(x|z)}$, and the evidence $\color{teal}{p_\theta(x)}$.

#### The Challenge: Intractable Posterior

The posterior $\color{red}{p_\theta(z|x)}$ is **intractable** to compute directly because it requires knowing $\color{teal}{p_\theta(x)}$, which involves the difficult integral we discussed earlier. This is where the VAE's two-part architecture comes in:

::: {.callout-tip icon=false}
## The Encoder (Recognition Network)

The **encoder** approximates the intractable posterior $\color{red}{p_\theta(z|x)}$ using variational inference.

- We introduce a **variational distribution** $\color{green}{q_\phi(z|x)}$ that is designed to be tractable
- The encoder learns parameters $\color{green}{\phi}$ to make $\color{green}{q_\phi(z|x)} \approx \color{red}{p_\theta(z|x)}$ as close as possible
- **Role**: Given an input $x$, the encoder outputs a distribution over likely latent codes $z$
:::

::: {.callout-tip icon=false}
## The Decoder (Generative Network)

The **decoder** models the likelihood $\color{blue}{p_\theta(x|z)}$, which is the generative part of the model.

- The decoder learns parameters $\color{blue}{\theta}$ to map from the latent space back to the data space
- **Role**: Given a latent code $z$, the decoder outputs a distribution over possible reconstructions $x$
:::

In summary: the encoder **compresses** observations $x$ into latent representations $z$, while the decoder **reconstructs** observations from latent codes.

### How can we learn $\color{green}{\phi}$ and $\color{blue}{\theta}$ jointly?

Now we have two sets of parameters to optimize:

- **Encoder parameters** $\color{green}{\phi}$: control the approximate posterior $\color{green}{q_\phi(z|x)}$
- **Decoder parameters** $\color{blue}{\theta}$: control the likelihood $\color{blue}{p_\theta(x|z)}$ (and also appear in the true posterior $\color{red}{p_\theta(z|x)}$)

Our goal is to make the estimated posterior $\color{green}{q_\phi(z|x)}$ as close as possible to the true (but intractable) posterior $\color{red}{p_\theta(z|x)}$.

#### Measuring Closeness: The KL Divergence

To measure how "close" two probability distributions are, we use the **Kullback-Leibler (KL) divergence**. Specifically, we use the **reverse KL divergence**:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \mathbb{E}_{z \sim \color{green}{q_\phi(z|x)}} \left[ \log \frac{\color{green}{q_\phi(z|x)}}{\color{red}{p_\theta(z|x)}} \right]$$

which can be written more explicitly as:

- For **discrete** latent variables:
$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \sum_{z \in Z} \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{\color{red}{p_\theta(z|x)}} \right)$$

- For **continuous** latent variables:
$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{\color{red}{p_\theta(z|x)}} \right) dz$$

::: {.callout-important icon=false}
## Key Properties of KL Divergence

Understanding these properties is crucial for grasping how VAE training works:

**1. Non-negativity:** $\text{KL}(q \; || \; p) \geq 0$ for any two distributions $q$ and $p$

   - The KL divergence is always non-negative or zero
   - This property is fundamental and comes from Jensen's inequality
   - Intuitively: there's always some "cost" to using an approximation unless it's exact

**2. Zero if and only if distributions are identical:** $\text{KL}(q \; || \; p) = 0 \iff q = p$ (almost everywhere)

   - When $q$ and $p$ are exactly the same, there's no information loss
   - This is our ideal case: $\color{green}{q_\phi(z|x)} \color{black}{=} \color{red}{p_\theta(z|x)}$
   - In practice, we get close but rarely achieve exactly zero

**3. Asymmetric (not a distance metric):** $\text{KL}(q \; || \; p) \neq \text{KL}(p \; || \; q)$ in general

   - The order matters! $\text{KL}(\color{green}{q_\phi} \; || \; \color{red}{p_\theta})$ is called the **reverse KL**
   - Reverse KL encourages $q$ to focus on regions where $p$ has high probability
   - This is why VAE tends to produce "mode-covering" behavior rather than "mode-seeking"

**4. Measures information loss:** It quantifies the expected extra bits (or nats) needed when using $q$ to encode samples from $p$

   - In VAE context: how much information we lose by using $\color{green}{q_\phi(z|x)}$ instead of the true $\color{red}{p_\theta(z|x)}$
:::

**Our Training Objective:**

By **minimizing** $\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right)$, we make our encoder's approximate posterior as accurate as possible. However, there's a problem: we can't directly compute this KL divergence because it involves the intractable posterior $\color{red}{p_\theta(z|x)}$! 

This is where the ELBO (Evidence Lower Bound) comes in, which we'll derive next.

### Deriving the Evidence Lower Bound (ELBO)

#### The Problem We Need to Solve

Recall that we want to minimize $\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right)$, but we can't compute it directly because it involves the intractable posterior $\color{red}{p_\theta(z|x)}$. 

The variational inference idea is that we can derive an **alternative objective** that:

1. Is tractable to compute
2. Still allows us to optimize both $\color{green}{\phi}$ and $\color{blue}{\theta}$
3. Automatically handles the intractability

Let's derive this alternative objective step by step.

#### Step 1: Start with the KL Divergence

For continuous latent variables, the KL divergence is defined as:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{\color{red}{p_\theta(z|x)}} \right) dz$$

#### Step 2: Apply Bayes' Rule to the Posterior

Using Bayes' rule, we know that:

$$\color{red}{p_\theta(z|x)} = \frac{p_\theta(x,z)}{\color{teal}{p_\theta(x)}}$$

Substituting this into our KL divergence:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)} \cdot \color{teal}{p_\theta(x)}}{p_\theta(x,z)} \right) dz$$

#### Step 3: Split the Logarithm

Using the property $\log(ab/c) = \log(a) + \log(b) - \log(c)$:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \int \color{green}{q_\phi(z|x)} \left[ \log(\color{teal}{p_\theta(x)}) + \log \left( \frac{\color{green}{q_\phi(z|x)}}{p_\theta(x,z)} \right) \right] dz$$

#### Step 4: Separate the Integral

Split into two integrals:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \int \color{green}{q_\phi(z|x)} \log(\color{teal}{p_\theta(x)}) \, dz + \int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{p_\theta(x,z)} \right) dz$$

::: {.callout-note icon=false}
## Key Observation: The Evidence is Constant

The first integral simplifies because $\color{teal}{p_\theta(x)}$ doesn't depend on $z$:

$$\int \color{green}{q_\phi(z|x)} \log(\color{teal}{p_\theta(x)}) \, dz = \log(\color{teal}{p_\theta(x)}) \int \color{green}{q_\phi(z|x)} \, dz = \log(\color{teal}{p_\theta(x)})$$

since $\int \color{green}{q_\phi(z|x)} \, dz = 1$ (probability distributions must integrate to 1).
:::

So now we have:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \log(\color{teal}{p_\theta(x)}) + \int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{p_\theta(x,z)} \right) dz$$

#### Step 5: Decompose the Joint Distribution

The joint distribution can be factored as:

$$p_\theta(x,z) = \color{blue}{p_\theta(x|z)} \cdot \color{purple}{p_\theta(z)}$$

Substituting this:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \log(\color{teal}{p_\theta(x)}) + \int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{\color{blue}{p_\theta(x|z)} \cdot \color{purple}{p_\theta(z)}} \right) dz$$

#### Step 6: Simplify Using Logarithm Properties

Using $\log(a/(bc)) = \log(a/b) - \log(c)$:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \log(\color{teal}{p_\theta(x)}) + \int \color{green}{q_\phi(z|x)} \left[ \log \left( \frac{\color{green}{q_\phi(z|x)}}{\color{purple}{p_\theta(z)}} \right) - \log(\color{blue}{p_\theta(x|z)}) \right] dz$$

#### Step 7: Split Into Two Integrals

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \log(\color{teal}{p_\theta(x)}) + \int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{\color{purple}{p_\theta(z)}} \right) dz - \int \color{green}{q_\phi(z|x)} \log(\color{blue}{p_\theta(x|z)}) \, dz$$

#### Step 8: Recognize Standard Forms

The second term is the KL divergence between $\color{green}{q_\phi(z|x)}$ and the prior $\color{purple}{p_\theta(z)}$:

$$\int \color{green}{q_\phi(z|x)} \log \left( \frac{\color{green}{q_\phi(z|x)}}{\color{purple}{p_\theta(z)}} \right) dz = \text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{purple}{p_\theta(z)}\right)$$

The third term is an expectation under $\color{green}{q_\phi(z|x)}$:

$$\int \color{green}{q_\phi(z|x)} \log(\color{blue}{p_\theta(x|z)}) \, dz = \mathbb{E}_{z \sim \color{green}{q_\phi(z|x)}} \left[ \log(\color{blue}{p_\theta(x|z)}) \right]$$

Therefore:

$$\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) = \log(\color{teal}{p_\theta(x)}) + \text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{purple}{p_\theta(z)}\right) - \mathbb{E}_{z \sim \color{green}{q_\phi(z|x)}} \left[ \log(\color{blue}{p_\theta(x|z)}) \right]$$

#### Step 9: Rearrange to Isolate the Evidence

Rearranging the equation:

$$\log(\color{teal}{p_\theta(x)}) = \text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right) + \underbrace{\mathbb{E}_{z \sim \color{green}{q_\phi(z|x)}} \left[ \log(\color{blue}{p_\theta(x|z)}) \right] - \text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{purple}{p_\theta(z)}\right)}_{\text{ELBO}(\color{blue}{\theta}, \color{green}{\phi}; x)}$$

::: {.callout-important icon=false}
## The Evidence Lower Bound (ELBO)

We define the **Evidence Lower Bound** (ELBO) as:

$$\text{ELBO}(\color{blue}{\theta}, \color{green}{\phi}; x) = \mathbb{E}_{z \sim \color{green}{q_\phi(z|x)}} \left[ \log(\color{blue}{p_\theta(x|z)}) \right] - \text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{purple}{p_\theta(z)}\right)$$

**Why is it called a "lower bound"?**

From the equation above:
$$\log(\color{teal}{p_\theta(x)}) = \underbrace{\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right)}_{\geq 0} + \text{ELBO}(\color{blue}{\theta}, \color{green}{\phi}; x)$$

Since $\text{KL} \geq 0$, we have:
$$\text{ELBO}(\color{blue}{\theta}, \color{green}{\phi}; x) \leq \log(\color{teal}{p_\theta(x)})$$

The ELBO provides a **lower bound** on the log-evidence $\log(\color{teal}{p_\theta(x)})$!
:::

#### Step 10: The VAE Loss Function

In practice, we want to **minimize a loss function**. The VAE loss is simply the **negative ELBO**:

$$\mathcal{L}_{\text{VAE}}(\color{blue}{\theta}, \color{green}{\phi}; x) = -\text{ELBO}(\color{blue}{\theta}, \color{green}{\phi}; x)$$

$$\boxed{\mathcal{L}_{\text{VAE}}(\color{blue}{\theta}, \color{green}{\phi}; x) = \text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{purple}{p_\theta(z)}\right) - \mathbb{E}_{z \sim \color{green}{q_\phi(z|x)}} \left[ \log(\color{blue}{p_\theta(x|z)}) \right]}$$

We seek the optimal parameters:

$$\color{green}{\phi}^*, \color{blue}{\theta}^* = \arg\min_{\color{green}{\phi}, \color{blue}{\theta}} \mathcal{L}_{\text{VAE}}(\color{blue}{\theta}, \color{green}{\phi}; x)$$

::: {.callout-tip icon=false}
## Interpreting the VAE Loss

The VAE loss has two terms:

1. **KL Regularization Term**: $\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{purple}{p_\theta(z)}\right)$
   - Encourages the encoder's approximate posterior to stay close to the prior
   - Acts as a regularizer preventing overfitting
   - Ensures the latent space has a nice structure

2. **Reconstruction Term**: $-\mathbb{E}_{z \sim \color{green}{q_\phi(z|x)}} \left[ \log(\color{blue}{p_\theta(x|z)}) \right]$
   - Encourages the decoder to reconstruct the input accurately
   - When the decoder outputs Gaussian distributions, this becomes MSE (Mean Squared Error)
   - When the decoder outputs Bernoulli distributions, this becomes BCE (Binary Cross-Entropy)

It enforces the following properties:

- **Structured latent geometry:** The KL term shapes latent space into a smooth, continuous manifold aligned with the prior.
- **Smooth interpolation:** Nearby latent points decode to semantically similar outputs, enabling meaningful interpolations.
- **Valid sampling:** Samples drawn from the prior fall in regions the decoder understands, producing realistic generations.
:::


**Parameter Names:**

- $\color{green}{\phi}$ are the **variational parameters** (encoder parameters)
- $\color{blue}{\theta}$ are the **generative parameters** (decoder parameters)

**Why This Solves Our Problem:**

Maximizing the ELBO (equivalently, minimizing $\mathcal{L}_{\text{VAE}}$) simultaneously:

1. Increases $\log(\color{teal}{p_\theta(x)})$ (generates more realistic samples)
2. Decreases $\text{KL}\left(\color{green}{q_\phi(z|x)} \; || \; \color{red}{p_\theta(z|x)}\right)$ (better posterior approximation)

And we can compute everything in the ELBO without ever needing the intractable $\color{red}{p_\theta(z|x)}$!

### Key Takeaways and Final Remarks

The Variational Autoencoder framework provides a principled solution to learning latent variable models when exact inference is intractable. By introducing a tractable variational posterior and optimizing the Evidence Lower Bound (ELBO), we transform an otherwise impossible likelihood maximization problem into a practical and scalable objective that can be optimized with stochastic gradient methods.

From the derivation, several core insights emerge:

- First, the VAE objective naturally decomposes into two competing terms: 
  - A reconstruction term that encourages faithful data generation, via the decoder. 
  - A KL regularization term that shapes the latent space by keeping the approximate posterior close to the prior, via the encoder. 
  - This trade-off is not an implementation detail, but a direct consequence of variational inference. 

- Second, maximizing the ELBO simultaneously improves data likelihood and posterior approximation, even though the true posterior is never computed explicitly.

This framework extends seamlessly to Conditional VAEs by conditioning both the encoder and decoder on auxiliary information. The mathematical structure of the ELBO remains unchanged, highlighting the flexibility and generality of variational inference as a modeling paradigm.