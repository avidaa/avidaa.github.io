# Modelling DNA Encoded Libraries (DELs)

## What Is the Aim of This Blog?

This blog aims to design and build an attachment-aware, synthon-level GNN for DNA-encoded library (DEL) data, extending compositional models like DEL-Compose by explicitly encoding where each synthon attaches, rather than treating synthons as context-free fragments.

## Introduction: What Are DNA Encoded Libraries (DELs)?

DNA encoded libraries (DELs) are large collections of small molecules, each tagged with a unique DNA barcode that records its synthetic history. They enable high throughput screening by pooling and sequencing to identify binders to a target. Watch [this](https://www.youtube.com/watch?v=8jLIgTBJmdU) short vidoe that explains DELs in a nutshell.

## Modelling DELs

### What is a correct likelihood function for DELs?

A DEL experiment involves several key stochastic steps that directly impact the statistical modeling of observed counts:

-   **Library synthesis**: initial molecule counts for each barcode can vary, introducing variability from the outset.
-   **Selection or binding step**: enriches certain barcodes based on their binding affinity, leading to barcode-specific changes in abundance.
-   **PCR amplification**: contributes substantial multiplicative noise and is a major source of overdispersion in the data.
-   **Sequencing**: captures a random, approximately Poisson-distributed sample from the amplified pool.

Thus, the observed DEL counts for each barcode are the outcome of a sequence of random processes, with amplification especially contributing to count heterogeneity.

#### Common likelihood functions for DELs

Below are common likelihood choices for DEL count data. In practice, DEL data are often **overdispersed** (variance \> mean) due to PCR/amplification and other multiplicative factors, and may also exhibit **excess zeros** (true absence, synthesis failures, or aggressive filtering).

##### Poisson

$$
X_i \sim \text{Poisson}(\lambda_i)
$$

Because sequencing counts in DEL experiments arise from random sampling of amplified DNA molecules, the sequencing step can be modeled using a Poisson distribution.

References:

- [Machine learning on DNA-encoded library count data using an uncertainty-aware probabilistic loss function](https://pmc.ncbi.nlm.nih.gov/articles/PMC10830332/)
- [Randomness in DNA Encoded Library Selection Data Can Be Modeled for More Reliable Enrichment Calculation](https://pubmed.ncbi.nlm.nih.gov/29437521/)

::: {.callout-note icon="false" collapse="true"}
## When to use Poisson?

Use Poisson when counts are well-explained by a single rate and the **mean–variance relationship is close to** $\mathrm{Var}(X) \approx \mathbb{E}[X]$ after accounting for obvious covariates (e.g., sequencing depth / library size).

Notice that Poisson is usually too narrow for DEL data once PCR noise is present.
:::

##### Poisson-Gamma or Negative Binomial (NB)

$$
X_i \mid \lambda_i \sim \text{Poisson}(\lambda_i), \qquad \lambda_i \sim \text{Gamma}(\alpha, \beta)
$$
<div style="text-align: center;">
or
</div>

$$
X_i \sim \text{NB}(\mu_i, \phi)
$$

Because DEL sequencing readouts often exhibit overdispersion due to amplification and selection variability, barcode counts can be modeled using a Negative Binomial distribution.

References:

- [Partial Product Aware Machine Learning on DNA-Encoded Libraries](https://arxiv.org/abs/2205.08020)
- [Discovery of TNF inhibitors from a DNA-encoded chemical library based on diels-alder cycloaddition](https://pubmed.ncbi.nlm.nih.gov/19875081/) - Figure 3.

::: {.callout-note icon="false" collapse="true"}
## When to use Negative Binomial?

Use NB when you see **variance increasing faster than the mean**, typically well-captured by $\mu + \phi\mu^2$. This is often the default for DEL counts because amplification introduces extra variability.

If you model *paired* pre-/post-selection counts, NB is often used for each condition with shared/related dispersion:

$$
X_i^\text{pre} \sim \text{NB}(\mu_i^\text{pre}, \phi)
$$

$$
X_i^\text{post} \sim \text{NB}(\mu_i^\text{post}, \phi)
$$

The dispersion parameter $\phi$ represents **technical variability** not biological variability.

Because:

-   PCR process is similar between pre and post experiments
-   Sequencing instrument is probably the same between pre and post experiments
-   Library chemistry is the same between pre and post experiments

it is a reasonable assumption that the dispersion parameter is approximately the same between pre and post experiments.

$$
\phi^\text{pre} \approx \phi^\text{post} = \phi
$$

Or, use a common prior for the dispersion parameter:

$$
\phi^\text{pre}, \phi^\text{post} \sim \text{common prior}
$$
:::

##### Zero-Inflated Models

$$
X_i \sim \text{ZIP}(\lambda_i, \pi_i)
$$

<div style="text-align: center;">
or 
</div>

$$
X_i \sim \text{ZINB}(\mu_i, \phi, \pi_i)
$$

Which assumes that there are two mechanisms that produce zeros:

$$
X_i = 0 \quad \text{with probability } \pi_i
$$

<div style="text-align: center;">
and
</div>

$$
X_i \sim \text{Poisson}(\lambda_i) \quad \text{with probability } 1 - \pi_i \quad \text{(for ZIP)}
$$

<div style="text-align: center;">
or 
</div>

$$
X_i \sim \text{NB}(\mu_i, \phi) \quad \text{with probability } 1 - \pi_i \quad \text{(for ZINB)}
$$


The interpretation of ZIP, ZINB is that with probability $\pi_i$, the observation is a **structural zero** (e.g., synthesis failure / true absence), **otherwise counts follow Poisson or Negative Binomial**. Zero inflated models should be treated as a *diagnostic driven choice*, **not a default likelihood choice**.

References:

- [DEL-Ranking: Ranking-Correction Denoising Framework for Elucidating Molecular Affinities in DNA-Encoded Libraries](https://openreview.net/forum?id=QfyZ28FpVY) - Check the reviewers comments.


::: {.callout-note icon="false" collapse="true"}
## When to use Zero-inflated models?

Zero inflated models assume that there are two mechanisms that produce zeros:

-   **Structural zeros**: these are zeros that are due to:
    -   Synthesis failure for certain barcodes
    -   Missing synthons or ligation errors for certain barcodes
    -   Amplification failure where barcode dropout during amplification
    -   Library filtering steps
    -   Compound absence in the selection pool
-   **Observed zeros**: these are zeros that are due to the fact that the molecule was not sequenced.

Structural zeros are conceptually different from **sampling zeros**, which Poisson or Negative Binomial can already explain. <span style="color: #d7263d;">This is important because many apparent extra zeros are already explained by **overdispersion** in the Negative Binomial model, *not* a separate zero-generation process.</span>

Zero inflated models are reasonable when:

1. Poisson or Negative Binomial alone underpredicts zeros in posterior predictive checks

    The diagnostic is to check if observed zero fraction is larger than simulated zero fraction from Poisson or Negative Binomial

2. Known synthesis failure mechanisms

    For instance:

    - specific synthons systematically fail
    - Library construction known to have dropout
    - Certain cycles prosuce missing compounds

    These are structural zeros that are not explained by overdispersion.

3. Exteremly sparse libraries

    If most barcodes are absent or filtered out, then ZIP, ZINB is a better fit than Poisson or Negative Binomial. 

Notice that:

- If sparsity is **already explained by low counts**, then <span style="color: #d7263d; font-weight: bold;">ZIP, ZINB is not necessary</span>.
- If **filtering is the main source of zeros**, then <span style="color: #d7263d; font-weight: bold;">ZIP, ZINB is not necessary</span>.

If zeros are mainly caused by *thresholding / filtering* (i.e., you only record nonzeros above a cutoff), a **hurdle model** may be a better conceptual fit than ZIP, ZINB.

:::

##### Poisson-Lognormal

In the Negative Binomial model, the rate parameter is assumed to vary according to a gamma distribution. In contrast, the Poisson-Lognormal model assumes that the logarithm of the rate parameter follows a normal (lognormal) distribution.

$$
X_i \mid \lambda_i \sim \text{Poisson}(\lambda_i), \qquad \lambda_i \sim \text{Lognormal}(\mu_i, \sigma^2)
$$

A lognormal model for the rate has **heavier tails than a gamma model**, so it tends to fit DEL data better when a small number of barcodes show very large enrichment.

References:

- There is **no well-known DEL paper** that explicitly uses a Poisson-Lognormal likelihood, although the model is statistically can be used for DEL data.

::: {.callout-note icon="false" collapse="true"}
## When to use Poisson-Lognormal?

Use Poisson–lognormal when noise is naturally **multiplicative on the rate** (PCR / assay effects), giving a log-normal-ish spread in effective rates across observations.

:::

##### Multinomial / Dirichlet-Multinomial

If we model counts **conditional on a fixed total read depth** (library size) in a sample:

$$
\mathbf{X} \sim \text{Multinomial}(N, \mathbf{p})
$$

with $N$ being the total read depth $(N = \sum_{i=1}^K X_i)$, where $K$ is the number of barcodes, and $\mathbf{p}$ being the vector of proportions of each barcode $(p_i = X_i / N)$.

A more overdispersed alternative is Dirichlet-multinomial.

$$
\mathbf{X} \mid \mathbf{p} \sim \text{Multinomial}(N, \mathbf{p}), \qquad \mathbf{p} \sim \text{Dirichlet}(\alpha)
$$

References:

- There is **no well-known DEL paper** that explicitly uses a Multinomial / Dirichlet-multinomial likelihood, although the model is statistically valid and can be used for DEL data. Check <span style="color:#d7263d;">When to use Multinomial / Dirichlet-multinomial?</span> for more details.

::: {.callout-note icon="false" collapse="true"}
## When to use Multinomial / Dirichlet-multinomial?

Use these when primary variability is driven by **relative composition** under a fixed (or modeled) total depth. This is common when comparing barcodes within the same sequencing run.

**In practice:**

- Multinomial can be too narrow; Dirichlet-multinomial adds extra variability in proportions. 

-  Multinomial / Dirichlet-multinomial are especially natural for modeling *within-sample* composition, whereas Poisson and Negative Binomial are common for *per-barcode* marginal modeling.
    - **Within-sample perspective:** Given one sample with total reads $N$, how are those reads split across all barcodes?
        - Model the joint vector of proportions/counts in that sample using a Multinomial distribution.
    - **Per-barcode perspective:** For barcode $i$, what count do I expect?
        - Model each barcode’s count distribution individually (often approximately independent, especially in sparse DEL settings).
    
- Most DEL models don't use Multinomial / Dirichlet-multinomial likelihood, mostly for practical reasons:

    1. Libraries are extremely large and have millions of barcodes. A multinomial likelihood becomes computationally difficult.
    2. [When counts are small relative to $N$, then $\text{Multinomial} \approx \text{independent Poisson}$.]{.highlight-key-point}

**Intuition:**

- **Multinomial perspective:** Imagine you have a total of $N$ sequencing reads, and you assign each one to a barcode. The barcodes share the total pool of reads.

- **Negative Binomial/Poisson perspective:** Instead, think of each barcode as producing its own number of reads independently, according to its own rate. The total number of reads is simply the sum of these independent counts.

::: {.callout-tip icon="false" collapse="true"}
## Why does $\text{Multinomial} \approx$ independent Poisson when counts are small relative to $N$?

Let $\mathbf{X}=(X_1,\dots,X_K) \sim \text{Multinomial}(N,\mathbf{p})$, where $\sum_{i=1}^K p_i = 1$ and $\sum_{i=1}^K X_i = N$.

For a fixed barcode $i$, the multinomial marginal is binomial:

$$
X_i \sim \text{Binomial}(N,p_i), \qquad \mathbb{E}[X_i]=Np_i
$$

So $Np_i$ is the expected read count for barcode $i$: among $N$ reads, each read lands in barcode $i$ with probability $p_i$.

In the DEL rare-event / large-depth regime, we consider:

- $N$ is large,
- Each barcode probability $p_i$ is small,
- $Np_i$ stays finite (typically on the order of 1).

Under this scaling ($N\to\infty$, $p_i\to 0$, $Np_i\to\text{fixed value}$), and assuming that Poisson's parameter, $\lambda_i$, is equal to $Np_i$, each marginal converges as 
$$
X_i \Rightarrow \text{Poisson}(\lambda_i), \quad \lambda_i = Np_i.
$$


::: {.callout-tip icon="false" collapse="true"}
## Why under these conditions, the binomial distribution converges to a Poisson distribution?

The probability mass function (PMF) of the Binomial distribution is:
$$
\Pr(X_i=k)=\binom{N}{k}p_i^k(1-p_i)^{N-k}
$$

Where $N$ is the total number of trials (or our experiment's read depth), $k$ is the number of successes (or our read count for a given barcode), and $p_i$ is the probability of success on each trial (or our barcode enrichment probability).

Now, let's recall the conditions of the rare-event/ large-depth regime:

- $N$ is large,
- Each barcode probability $p_i$ is small,
- $Np_i$ stays finite (typically on the order of 1).

We stated that under this scaling, and **assuming that Poisson's parameter, $\lambda_i$, is equal to $Np_i$**, the PMF of the binomial distribution converges to the PMF of a Poisson distribution.

Let's start by rewriting $p_i$ as:
$$p_i = \frac{\lambda_i}{N},$$

then the PMF of the binomial distribution becomes:

$$
\Pr(X_i=k)=\binom{N}{k}\left(\frac{\lambda_i}{N}\right)^k\left(1-\frac{\lambda_i}{N}\right)^{N-k}.
$$

Recall that: 
$$\binom{N}{k} = \frac{N!}{k!(N-k)!}$$
and
$$N! = N(N-1)(N-2)(N-3)\cdots 3\times 2\times 1.$$
It can be written as 
$$N! = N(N-1)\cdots(N-k+1)(N-k)!$$ 
This notation is useful because it shows that in $\binom{N}{k} = \frac{N!}{k!(N-k)!}$, the $(N-k)!$ in numerator and denominator cancel out each other. Hence, we can rewrite the binomial coefficient as:
$$
\binom{N}{k} = \frac{N(N-1)\cdots(N-k+1)}{k!}.
$$

Therefore, the PMF of the binomial distribution becomes:
$$
\Pr(X_i=k)=\binom{N}{k}\left(\frac{\lambda_i}{N}\right)^k\left(1-\frac{\lambda_i}{N}\right)^{N-k}
=
\frac{N(N-1)\cdots(N-k+1)}{k!}\frac{\lambda_i^k}{N^k}\left(1-\frac{\lambda_i}{N}\right)^{N-k}.
$$

Let's focus on the first two terms:

$$
\frac{N(N-1)\cdots(N-k+1)}{k!}\frac{\lambda_i^k}{N^k}
$$

This can be written as:
$$
\frac{\lambda_i^k}{k!}\cdot
\frac{N}{N}\cdot\frac{N-1}{N}\cdots\frac{N-k+1}{N}
$$

If we set $\frac{\lambda_i^k}{k!}$ aside, then the product of the remaining terms, can be written as a product of $k$ terms (because $N^k$ in denominator), each of which is of the form $\frac{N-j}{N}$ for $j=0,1,2,\dots,k-1$ ($k-1$, because the first term is $\frac{N}{N}$), hence:
$$
\frac{N}{N}\cdot\frac{N-1}{N}\cdots\frac{N-k+1}{N} = \prod_{j=0}^{k-1}\left(1-\frac{j}{N}\right)
$$

Now, let's bring in the first term back, we get:
$$
\frac{\lambda_i^k}{k!}\cdot \prod_{j=0}^{k-1}\left(1-\frac{j}{N}\right)
$$

In the rare-event/ large-depth regime, as $N\to\infty$, each term goes to 1, i.e., 

$$
\left(1-\overset{\longrightarrow \ 0}{\cancel{\frac{j}{N}}}\right)\longrightarrow 1 \quad (N\to\infty),
$$

That is because $k$ is fixed (it does not grow with $N$). Thus, $\frac{j}{N}\to 0$ for each $j=0,1,2,\dots,k-1$. Hence, the product of the remaining terms tends to $1$:

$$
\prod_{j=0}^{k-1}\left(1-\overset{\longrightarrow \ 0}{\cancel{\frac{j}{N}}}\right)\longrightarrow 1.
$$

Therefore, in the rare-event/ large-depth regime, the first two terms of the PMF of the binomial distribution converge to:



$$
\boxed{\binom{N}{k}p_i^k = \binom{N}{k}\left(\frac{\lambda_i}{N}\right)^k = \frac{N(N-1)\cdots(N-k+1)}{k!}\frac{\lambda_i^k}{N^k} = \frac{\lambda_i^k}{k!}.\overset{\longrightarrow\,1}{\cancel{\prod_{j=0}^{k-1}\left(1-\overset{\longrightarrow \ 0}{\cancel{\frac{j}{N}}}\right)}} = \frac{\lambda_i^k}{k!}.1 = \frac{\lambda_i^k}{k!}}
$$




Now, let's focus on the third term:

$$
\left(1-\frac{\lambda_i}{N}\right)^{N-k}
$$

This term can be written as:

$$
\left(1-\frac{\lambda_i}{N}\right)^N
\left(1-\frac{\lambda_i}{N}\right)^{-k}
$$

Recall that (this is a well-known result):
$$
\lim_{N\to\infty} \left(1-\frac{a}{N}\right)^N = e^{-a}
$$

Therefore as $N\to\infty$, the first term converges to $e^{-\lambda_i}$:
$$
\lim_{N\to\infty} \left(1-\frac{\lambda_i}{N}\right)^N = e^{-\lambda_i}
$$

The second term converges to $1$ as $N\to\infty$:

$$
\lim_{N\to\infty} \left(1-\overset{\longrightarrow\,0}{\cancel{\frac{\lambda_i}{N}}}\right)^{-k} = 1
$$

That is because $k$ is fixed (it does not grow with $N$). Thus, $\frac{\lambda_i}{N}\to 0$ as $N\to\infty$. 

Putting it all together, the third term converges to:


$$
\boxed{\left(1-\frac{\lambda_i}{N}\right)^{N-k} = \left(1-\frac{\lambda_i}{N}\right)^N
\left(1-\frac{\lambda_i}{N}\right)^{-k} = e^{-\lambda_i}\cdot 1 = e^{-\lambda_i}}
$$



Let's bring in the result of the first and second term back, which was $\frac{\lambda_i^k}{k!}$. Now, we get:

$$
\boxed{\Pr(X_i=k) = e^{-\lambda_i}\frac{\lambda_i^k}{k!}}
$$

What is a PMF of a Poisson distribution?
$$
\boxed{\Pr(X_i=k)=e^{-\lambda_i}\frac{\lambda_i^k}{k!}}
$$

Therefore, the PMF of the binomial distribution converges to the PMF of a Poisson distribution:

$$
\Pr(X_i=k)\longrightarrow e^{-\lambda_i}\frac{\lambda_i^k}{k!} = \Pr(X_i=k)\text{ (PMF of Poisson distribution)}.
$$
:::

:::{.callout-tip icon="false" collapse="true"}

## Why independent Poisson?

Jointly, the multinomial constraint $\sum_i X_i=N$ induces weak negative dependence between barcodes. 

Intuitively, the total number of reads is a fixed budget: if one barcode receives more reads than expected, fewer reads are available for the others. In a mathematical sense, the covariance between two barcodes is negative in a multinomial model,

$$
\mathrm{Cov}(X_i,X_j)=-Np_ip_j \quad (i\neq j),
$$

The negative value indicates the inuition mentioned above: an increase in one variable often requires a decrease in the other, as the sum of all variables is fixed at $N$. So, the covariance is negative by construction. Equivalently, in terms of proportions $\hat p_i=X_i/N$,

$$
\mathrm{Cov}(\hat p_i,\hat p_j)=-\frac{p_ip_j}{N},
$$

which shrinks to $0$ as $N$ grows. In DEL, each $p_i$ is also very small, so these cross-barcode covariances are tiny. That is why the **dependence is usually negligible in practice**, and the joint distribution is well approximated by a product of **independent** Poisson variables:

$$
X_i \approx \text{Poisson}(\lambda_i), \qquad \lambda_i = Np_i,\quad i=1,\dots,K. \quad \text{(K is number of barcodes)}
$$

This is why DEL models often treat per-barcode counts as independent Poisson (or overdispersed Negative Binomial): it is an accurate and computationally convenient approximation in the sparse-count regime.

:::

:::
:::

:::{.callout-tip icon="false" collapse="true"}
## Binomial / Beta-Binomial to model per-barcode counts?

When modeling a single barcode’s counts conditional on fixed total read depth $N$, we can treat the number of reads assigned to barcode $i$ as the marginal of a multinomial model:

$$
\mathbf{X} \sim \text{Multinomial}(N,\mathbf{p})
\quad \Rightarrow \quad
X_i \sim \text{Binomial}(N,p_i)
$$

with $p_i$ the true proportion of barcode $i$ in the pool. Binomial assumes a fixed probability per read and variance $N p_i(1-p_i)$, which can be too narrow when enrichment probabilities vary across experiments, PCR amplification, or selection noise.

A more overdispersed alternative is the Beta-binomial.

References:

- [Quantitative Comparison of Enrichment from DNA-Encoded Chemical Library Selections](https://pubs.acs.org/doi/10.1021/acscombsci.8b00116) - Introduces a binomial distribution model to compute normalized enrichment z-scores from DEL selections.

$$
X_i \sim \text{BetaBinomial}(N, a_i, b_i)
$$

::: {.callout-note icon="false" collapse="true"}
## When is Beta–binomial useful for DEL?

The Beta prior adds **extra variability in enrichment probability**, analogous to how the Negative Binomial adds extra variability to Poisson rates.

**The Beta–binomial is the one-dimensional marginal of the Dirichlet–multinomial**: if one barcode is treated as “success” and all others as “failure,” the Dirichlet prior on proportions reduces to a Beta prior for that barcode.


Beta–binomial is particularly natural when modeling:

- enrichment of **one barcode vs the background library**
- **binder vs non‑binder probability** formulations
- selection experiments where **total sequencing depth is fixed**

**In practice:**

- In large DEL libraries, Beta–binomial can be easier to use than Dirichlet–multinomial because it avoids modeling all barcodes jointly.

- When counts are small relative to $N$, then $\text{BetaBinomial} \approx \text{Negative Binomial}$. 

**Intuition:**

- **Dirichlet–multinomial** models the *entire library composition*
- **Beta–binomial** models *one barcode’s share of reads*
:::

:::