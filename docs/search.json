[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Avid Afzal",
    "section": "",
    "text": "I am a Data Scientist specializing in computer engineering, bioinformatics, and cheminformatics. My work centers on leveraging Bayesian statistics, experimental design, and deep learning to address complex problems in data science. By integrating these domains, I aim to develop robust, innovative solutions for challenging real-world applications."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Here I share posts about topics that interest me and I am trying to learn, ranging from statistics and machine learning/AI to the drug discovery process. Click “Read more” to explore each post in detail."
  },
  {
    "objectID": "blogs.html#blog-post-title-1",
    "href": "blogs.html#blog-post-title-1",
    "title": "Blogs",
    "section": "",
    "text": "Published on: 2024-08-26\nThis is a brief preview of my first blog post. It discusses interesting topics such as Bayesian statistics, deep learning, and more.\nRead more"
  },
  {
    "objectID": "blogs.html#blog-post-title-2",
    "href": "blogs.html#blog-post-title-2",
    "title": "Blogs",
    "section": "",
    "text": "Published on: 2024-08-20\nIn this post, I explore the applications of experimental design in bioinformatics, with real-world examples and case studies.\nRead more"
  },
  {
    "objectID": "blogs/ODE_Gen_AI.html",
    "href": "blogs/ODE_Gen_AI.html",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "",
    "text": "Differential equation is a mathematical equation that relates a function to its derivative.\n\nWhat does a derivative of a function tell us?\n\n\n\nIt tells us the rate of change of the function.\n\n\n\nExample:\n\n\n\nA function \\(f\\) represents some quantity of interest, e.g., temperature, cell count or concentration of a drug.\n\n\n\n\nThe derivative of \\(f\\) represents the rate of change of that quantity. That is, how fast a temperature is rising or how fast a population is growing.\n\n\n\n\n\nOrdinary Differential Equations (ODEs)\nStochastic Differential Equations (SDEs)\nPartial Differential Equations (PDEs)\n\n\n\n\nODEs involve functions of a single variable and their derivatives (such as time or space).\nFor example, consider a process where a variable \\(x\\) changes over time: \\[ \\frac{dx(t)}{dt} = f(x(t), t, \\theta)\\]\n\nThis equation means that a small change in \\(x\\) (denoted \\(dx\\)) over a small time interval \\(dt\\) is determined by a function \\(f(x(t), t, \\theta)\\).\nThe function \\(f\\), parameterised by \\(\\theta\\), depends on the current value of \\(x(t)\\) and the current time \\(t\\).\nIf we know the starting value of \\(x\\) at time \\(t_0\\), we can use this equation repeatedly to calculate the value of \\(x\\) at later times, such as \\(x(t_1)\\). \\[x(t_1) = x(t_0) + \\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\]\n\n\n\n\nSometimes we can analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta)dt\\) term.\nFor example:\nLet’s solve the initial value problem:\n\\[\n\\frac{dx}{dt} = 4t^3 + 2t;\\quad x(0) = 0;\\quad {Find } x(2)\n\\]\nSolution:\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{green}{\\int_0^2 (4t^3 + 2t)\\, dt} \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 4 \\left[\\frac{t^4}{4}\\right]_0^2 + 2 \\left[\\frac{t^2}{2}\\right]_0^2 \\\\\n     &= \\left[t^4\\right]_0^2 + \\left[t^2\\right]_0^2 \\\\\n     &= (16 - 0) + (4 - 0) \\\\\n     &= \\boxed{20}\n\\end{aligned}\\]\nBut, in some cases we can not analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) term.\nFor example:\n\\[\n\\frac{dx}{dt} = \\cos(t^2);\\quad x(0) = 0;\\quad \\text{Find } x(2)\n\\]\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{red}{\\int_0^2 \\cos(t^2)\\, dt}\n\\end{aligned}\\]\nBut, the \\(\\int \\cos(t^2) dt\\) does not have a closed-form solution.\nIn cases where the \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) cannot be solved analytically, we can use approximation methods to estimate the integral, such as:\n\nEuler method\nRunge-Kutta mathods\n\n\n\n\n\n\nSDEs extend ODEs by incorporating randomness into the dynamics.\nThe ODE equation can be written as: \\[\ndx(t) = f(x(t), t, \\theta)dt\n\\]\nSDE adds a second term to this equation to introduces randomness: \\[\ndx(t) = f(x(t), t, \\theta)dt + g(x(t), t, \\theta) dW(t)\n\\]\n\nIn this equation $$\n\n\n\n\n\nPartial Differential Equations (PDEs):\n\nInvolve functions of multiple variables and their partial derivatives (e.g. space and time)"
  },
  {
    "objectID": "blogs/ODE_Gen_AI.html#what-are-the-types-of-differential-equations",
    "href": "blogs/ODE_Gen_AI.html#what-are-the-types-of-differential-equations",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "",
    "text": "Ordinary Differential Equations (ODEs)\nStochastic Differential Equations (SDEs)\nPartial Differential Equations (PDEs)\n\n\n\n\nODEs involve functions of a single variable and their derivatives (such as time or space).\nFor example, consider a process where a variable \\(x\\) changes over time: \\[ \\frac{dx(t)}{dt} = f(x(t), t, \\theta)\\]\n\nThis equation means that a small change in \\(x\\) (denoted \\(dx\\)) over a small time interval \\(dt\\) is determined by a function \\(f(x(t), t, \\theta)\\).\nThe function \\(f\\), parameterised by \\(\\theta\\), depends on the current value of \\(x(t)\\) and the current time \\(t\\).\nIf we know the starting value of \\(x\\) at time \\(t_0\\), we can use this equation repeatedly to calculate the value of \\(x\\) at later times, such as \\(x(t_1)\\). \\[x(t_1) = x(t_0) + \\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\]\n\n\n\n\nSometimes we can analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta)dt\\) term.\nFor example:\nLet’s solve the initial value problem:\n\\[\n\\frac{dx}{dt} = 4t^3 + 2t;\\quad x(0) = 0;\\quad {Find } x(2)\n\\]\nSolution:\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{green}{\\int_0^2 (4t^3 + 2t)\\, dt} \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 4 \\left[\\frac{t^4}{4}\\right]_0^2 + 2 \\left[\\frac{t^2}{2}\\right]_0^2 \\\\\n     &= \\left[t^4\\right]_0^2 + \\left[t^2\\right]_0^2 \\\\\n     &= (16 - 0) + (4 - 0) \\\\\n     &= \\boxed{20}\n\\end{aligned}\\]\nBut, in some cases we can not analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) term.\nFor example:\n\\[\n\\frac{dx}{dt} = \\cos(t^2);\\quad x(0) = 0;\\quad \\text{Find } x(2)\n\\]\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{red}{\\int_0^2 \\cos(t^2)\\, dt}\n\\end{aligned}\\]\nBut, the \\(\\int \\cos(t^2) dt\\) does not have a closed-form solution.\nIn cases where the \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) cannot be solved analytically, we can use approximation methods to estimate the integral, such as:\n\nEuler method\nRunge-Kutta mathods\n\n\n\n\n\n\nSDEs extend ODEs by incorporating randomness into the dynamics.\nThe ODE equation can be written as: \\[\ndx(t) = f(x(t), t, \\theta)dt\n\\]\nSDE adds a second term to this equation to introduces randomness: \\[\ndx(t) = f(x(t), t, \\theta)dt + g(x(t), t, \\theta) dW(t)\n\\]\n\nIn this equation $$\n\n\n\n\n\nPartial Differential Equations (PDEs):\n\nInvolve functions of multiple variables and their partial derivatives (e.g. space and time)"
  },
  {
    "objectID": "blogs/ODE_Gen_AI.html#modelling-the-dynamics-of-data",
    "href": "blogs/ODE_Gen_AI.html#modelling-the-dynamics-of-data",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Modelling the dynamics of data",
    "text": "Modelling the dynamics of data\nThat is a continuous transformation between simple distributions e.g. Guassian to a complex data distribution, e.g. image or RNA structure.\n\n\nVideo"
  },
  {
    "objectID": "blogs/ODE_Gen_AI.html#diffusion-models",
    "href": "blogs/ODE_Gen_AI.html#diffusion-models",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Diffusion models",
    "text": "Diffusion models"
  },
  {
    "objectID": "blogs/ODE_Gen_AI.html#flow-matching-models",
    "href": "blogs/ODE_Gen_AI.html#flow-matching-models",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Flow Matching models",
    "text": "Flow Matching models"
  },
  {
    "objectID": "blogs/ODE_Gen_AI.html#common-types-of-generative-ai-models",
    "href": "blogs/ODE_Gen_AI.html#common-types-of-generative-ai-models",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Common Types of Generative AI Models",
    "text": "Common Types of Generative AI Models\n\nGenerative Adversarial Networks (GANs)"
  },
  {
    "objectID": "blogs/ODE_Gen_AI.html#bibliography",
    "href": "blogs/ODE_Gen_AI.html#bibliography",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Bibliography",
    "text": "Bibliography\n\nContinuous Normalizing Flows. Vikram Voleti."
  },
  {
    "objectID": "blogs/ODE_Gen_AI/ODE_Gen_AI.html",
    "href": "blogs/ODE_Gen_AI/ODE_Gen_AI.html",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "",
    "text": "Differential equation is a mathematical equation that relates a function to its derivative.\n\nWhat does a derivative of a function tell us?\n\n\n\nIt tells us the rate of change of the function.\n\n\n\nExample:\n\n\n\nA function \\(f\\) represents some quantity of interest, e.g., temperature, cell count or concentration of a drug.\n\n\n\n\nThe derivative of \\(f\\) represents the rate of change of that quantity. That is, how fast a temperature is rising or how fast a population is growing.\n\n\n\n\n\nOrdinary Differential Equations (ODEs)\nStochastic Differential Equations (SDEs)\nPartial Differential Equations (PDEs)\n\n\n\n\nODEs involve functions of a single variable and their derivatives (such as time or space).\nFor example, consider a process where a variable \\(x\\) changes over time: \\[ \\frac{dx(t)}{dt} = f(x(t), t, \\theta)\\]\n\nThis equation means that a small change in \\(x\\) (denoted \\(dx\\)) over a small time interval \\(dt\\) is determined by a function \\(f(x(t), t, \\theta)\\).\nThe function \\(f\\), parameterised by \\(\\theta\\), depends on the current value of \\(x(t)\\) and the current time \\(t\\).\nIf we know the starting value of \\(x\\) at time \\(t_0\\), we can use this equation repeatedly to calculate the value of \\(x\\) at later times, such as \\(x(t_1)\\). \\[x(t_1) = x(t_0) + \\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\]\n\n\n\n\nSometimes we can analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta)dt\\) term.\nFor example:\nLet’s solve the initial value problem:\n\\[\n\\frac{dx}{dt} = 4t^3 + 2t;\\quad x(0) = 0;\\quad {Find } x(2)\n\\]\nHere, \\(f(x(t), t)=4t^3 + 2t\\). The solution is:\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{green}{\\int_0^2 (4t^3 + 2t)\\, dt} \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 4 \\left[\\frac{t^4}{4}\\right]_0^2 + 2 \\left[\\frac{t^2}{2}\\right]_0^2 \\\\\n     &= \\left[t^4\\right]_0^2 + \\left[t^2\\right]_0^2 \\\\\n     &= (16 - 0) + (4 - 0) \\\\\n     &= \\boxed{20}\n\\end{aligned}\\]\nBut, in some cases we can not analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) term.\nFor example:\n\\[\n\\frac{dx}{dt} = \\cos(t^2);\\quad x(0) = 0;\\quad \\text{Find } x(2)\n\\]\nHere, \\(f(x(t), t) = \\cos(t^2)\\). Hence, to solve for$x(2), we the following equation:\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{red}{\\int_0^2 \\cos(t^2)\\, dt}\n\\end{aligned}\\]\nBut, the \\(\\int \\cos(t^2) dt\\) does not have a closed-form solution.\nIn cases where the \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) cannot be solved analytically, we can use approximation methods to estimate the integral, such as:\n\nEuler method\nRunge-Kutta mathods\n\n\n\n\n\n\nSDEs extend ODEs by incorporating randomness into the dynamics.\nThe ODE equation can be written as: \\[\ndx(t) = f(x(t), t, \\theta)dt\n\\]\nSDE adds a second term to this equation to introduces randomness: \\[\ndx(t) = f(x(t), t, \\theta)dt + g(x(t), t, \\theta) dW(t)\n\\]\n\nIn this equation the first term, \\(f(x(t), t, \\theta)dt\\) is the drift term, same as ODE.\nThe second term, \\(g(x(t), t, \\theta) dW(t)\\) is the diffusion term, that introduces the randomness into the dynamics.\n\n\\(dW(t)\\) represents an increment of a Wiener process, also known as Brownian motion. It can be thought of as a random variable drawn from a normal distribution, such as:\n\\[dW(t) \\sim \\mathcal{N}(0, dt)\\]\n\n\n\n\n\n\nLangevin equation is an example of an SDE, developed by Paul Langevin to explore movements ( that, is dynamic) of individual particles suspended (that is, in equilibrium) in a fluid at a fix temperature.\n\n\\[\ndx(t) = - \\nabla_x U(x(t))dt + \\sigma dW(t)   \n\\]\n\nIn this equation, \\(U(x(t))\\) is a potential energy field. \\(\\nabla_x\\) is the gradient of the potential energy field and \\(\\sigma\\) is the diffusion term or diffusion strength.\n\\(\\sigma\\) is set to \\(\\sqrt{2}\\) due to Fluctuation-Dissipation Theorem (FDT), which states that the amount of random fluctuation (noise) in a system must be balanced by its dissipative forces (like friction or drag) if the system is to reach thermal equilibrium.\n\nThe fluctuations term is the \\(\\sigma dW(t)\\)\nThe dissipation term is the \\(- \\nabla_x U(x(t))dt\\)\nWhen the diffusion strength, \\(\\sigma\\), match the drift term, \\(- \\nabla_x U(x(t))\\), then the system reach thermal equilibrium.\nIn thermal equilibrium, the stationary distribution (that is, the probability distribution that remains unchanged over time as the system evolves), of the particle is known as the Gibbs (Boltzmann) distribution.\nThe Gibbs distribution is given by: \\[\np(x) \\propto e^{-U(x)}\n\\] If a particle moves according to the (overdamped) Langevin dynamics, where the noise and friction are balanced (as described by the SDE below), \\[\ndx(t) = - \\nabla_x U(x(t))dt + \\sqrt{2} dW(t)\n\\] then, as time goes to infinity (\\(t \\to \\infty\\)), the probability of finding the particle at position \\(x\\) approaches the Gibbs distribution. In other words, the Gibbs distribution describes the locations where the particle is most likely to be found in the long run.\n\n\nTODO: 1) Explain the discrete approximation and how is that related to gradient decent. 1.1) stochastic gradient descent is a stochastic approximation of gradient descent optimization. The stochasticity comes from mini-batching the data. At the end of the day it finds a point estimate (minimum). 1.2) Stochastic Gradient Langevin Dynamics (SGLD) is the discrete approximation of the Langevin equation. The discrete approximation of the Langevin equation is also called Euler–Maruyama method. 2) notice that sigma = sqrt{2} is called overdamped langevin equatuin: it is not sqrt{2} that leads to overdamped, it is including or excluding inertia that cause underdamped or overdamped system.\n\n\n\n\nPartial Differential Equations (PDEs):\n\nInvolve functions of multiple variables and their partial derivatives (e.g. space and time)\n\n ** This should be the Fokker-Planck Equation **"
  },
  {
    "objectID": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#what-are-the-types-of-differential-equations",
    "href": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#what-are-the-types-of-differential-equations",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "",
    "text": "Ordinary Differential Equations (ODEs)\nStochastic Differential Equations (SDEs)\nPartial Differential Equations (PDEs)\n\n\n\n\nODEs involve functions of a single variable and their derivatives (such as time or space).\nFor example, consider a process where a variable \\(x\\) changes over time: \\[ \\frac{dx(t)}{dt} = f(x(t), t, \\theta)\\]\n\nThis equation means that a small change in \\(x\\) (denoted \\(dx\\)) over a small time interval \\(dt\\) is determined by a function \\(f(x(t), t, \\theta)\\).\nThe function \\(f\\), parameterised by \\(\\theta\\), depends on the current value of \\(x(t)\\) and the current time \\(t\\).\nIf we know the starting value of \\(x\\) at time \\(t_0\\), we can use this equation repeatedly to calculate the value of \\(x\\) at later times, such as \\(x(t_1)\\). \\[x(t_1) = x(t_0) + \\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\]\n\n\n\n\nSometimes we can analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta)dt\\) term.\nFor example:\nLet’s solve the initial value problem:\n\\[\n\\frac{dx}{dt} = 4t^3 + 2t;\\quad x(0) = 0;\\quad {Find } x(2)\n\\]\nHere, \\(f(x(t), t)=4t^3 + 2t\\). The solution is:\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{green}{\\int_0^2 (4t^3 + 2t)\\, dt} \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 0 + \\int_0^2 4t^3\\, dt + \\int_0^2 2t\\, dt \\\\\n     &= 4 \\left[\\frac{t^4}{4}\\right]_0^2 + 2 \\left[\\frac{t^2}{2}\\right]_0^2 \\\\\n     &= \\left[t^4\\right]_0^2 + \\left[t^2\\right]_0^2 \\\\\n     &= (16 - 0) + (4 - 0) \\\\\n     &= \\boxed{20}\n\\end{aligned}\\]\nBut, in some cases we can not analytically integrate \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) term.\nFor example:\n\\[\n\\frac{dx}{dt} = \\cos(t^2);\\quad x(0) = 0;\\quad \\text{Find } x(2)\n\\]\nHere, \\(f(x(t), t) = \\cos(t^2)\\). Hence, to solve for$x(2), we the following equation:\n\\[\\begin{aligned}\nx(2) &= x(0) + \\textcolor{red}{\\int_0^2 \\cos(t^2)\\, dt}\n\\end{aligned}\\]\nBut, the \\(\\int \\cos(t^2) dt\\) does not have a closed-form solution.\nIn cases where the \\(\\int_{t_0}^{t_1} f(x(t), t, \\theta) dt\\) cannot be solved analytically, we can use approximation methods to estimate the integral, such as:\n\nEuler method\nRunge-Kutta mathods\n\n\n\n\n\n\nSDEs extend ODEs by incorporating randomness into the dynamics.\nThe ODE equation can be written as: \\[\ndx(t) = f(x(t), t, \\theta)dt\n\\]\nSDE adds a second term to this equation to introduces randomness: \\[\ndx(t) = f(x(t), t, \\theta)dt + g(x(t), t, \\theta) dW(t)\n\\]\n\nIn this equation the first term, \\(f(x(t), t, \\theta)dt\\) is the drift term, same as ODE.\nThe second term, \\(g(x(t), t, \\theta) dW(t)\\) is the diffusion term, that introduces the randomness into the dynamics.\n\n\\(dW(t)\\) represents an increment of a Wiener process, also known as Brownian motion. It can be thought of as a random variable drawn from a normal distribution, such as:\n\\[dW(t) \\sim \\mathcal{N}(0, dt)\\]\n\n\n\n\n\n\nLangevin equation is an example of an SDE, developed by Paul Langevin to explore movements ( that, is dynamic) of individual particles suspended (that is, in equilibrium) in a fluid at a fix temperature.\n\n\\[\ndx(t) = - \\nabla_x U(x(t))dt + \\sigma dW(t)   \n\\]\n\nIn this equation, \\(U(x(t))\\) is a potential energy field. \\(\\nabla_x\\) is the gradient of the potential energy field and \\(\\sigma\\) is the diffusion term or diffusion strength.\n\\(\\sigma\\) is set to \\(\\sqrt{2}\\) due to Fluctuation-Dissipation Theorem (FDT), which states that the amount of random fluctuation (noise) in a system must be balanced by its dissipative forces (like friction or drag) if the system is to reach thermal equilibrium.\n\nThe fluctuations term is the \\(\\sigma dW(t)\\)\nThe dissipation term is the \\(- \\nabla_x U(x(t))dt\\)\nWhen the diffusion strength, \\(\\sigma\\), match the drift term, \\(- \\nabla_x U(x(t))\\), then the system reach thermal equilibrium.\nIn thermal equilibrium, the stationary distribution (that is, the probability distribution that remains unchanged over time as the system evolves), of the particle is known as the Gibbs (Boltzmann) distribution.\nThe Gibbs distribution is given by: \\[\np(x) \\propto e^{-U(x)}\n\\] If a particle moves according to the (overdamped) Langevin dynamics, where the noise and friction are balanced (as described by the SDE below), \\[\ndx(t) = - \\nabla_x U(x(t))dt + \\sqrt{2} dW(t)\n\\] then, as time goes to infinity (\\(t \\to \\infty\\)), the probability of finding the particle at position \\(x\\) approaches the Gibbs distribution. In other words, the Gibbs distribution describes the locations where the particle is most likely to be found in the long run.\n\n\nTODO: 1) Explain the discrete approximation and how is that related to gradient decent. 1.1) stochastic gradient descent is a stochastic approximation of gradient descent optimization. The stochasticity comes from mini-batching the data. At the end of the day it finds a point estimate (minimum). 1.2) Stochastic Gradient Langevin Dynamics (SGLD) is the discrete approximation of the Langevin equation. The discrete approximation of the Langevin equation is also called Euler–Maruyama method. 2) notice that sigma = sqrt{2} is called overdamped langevin equatuin: it is not sqrt{2} that leads to overdamped, it is including or excluding inertia that cause underdamped or overdamped system.\n\n\n\n\nPartial Differential Equations (PDEs):\n\nInvolve functions of multiple variables and their partial derivatives (e.g. space and time)\n\n ** This should be the Fokker-Planck Equation **"
  },
  {
    "objectID": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#modelling-the-dynamics-of-data",
    "href": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#modelling-the-dynamics-of-data",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Modelling the dynamics of data",
    "text": "Modelling the dynamics of data\nThat is a continuous transformation between simple distributions e.g. Guassian to a complex data distribution, e.g. image or RNA structure.\n\n\nVideo"
  },
  {
    "objectID": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#diffusion-models",
    "href": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#diffusion-models",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Diffusion models",
    "text": "Diffusion models\nIt has two steps, forward process and reverse process.\n\nForward process\nIn the forward process we introduce noise. This noise is drawn from a normal distribution, with the following parameters: \\[\\Sigma_t = \\beta_t \\mathbf I\\] \\[\\mu_t = \\sqrt{1-\\beta_t} X_{t-1}\\]\nThat is:\n\\[\nq(X_t|X_{t-1}) = \\mathcal{N}(\\sqrt{1-\\beta_t} X_{t-1};\\quad \\beta_t \\mathbf I)\n\\]\nThere are \\(1, ..., T\\) steps between \\(X_0\\) to \\(X_1\\). Each added noise is independent from other added noises in other steps. Hence:\n\\[\nq(X_{1:T} | X_0) = \\prod_{t=1}^T q(X_t | X_{t-1})\n\\]\nThe symbol \\(\\textcolor{red}{:}\\) in \\(q(X_{1\\textcolor{red}{:}T} | X_0)\\), means that we appy \\(q\\) repeatedly from \\(t=1\\) to \\(t=T\\). This is also called  trajectory.\nLet’s assume \\(t=100\\), as it stands, to compute \\(X_{100}\\), we should compute \\(q(X)\\), 100 times to get to \\(X_{100}\\). Reparametrisation trick can help here.\n\n\nReverse process\nHow does SDE, SGM and DDPM are related to Diffusion model?\n\nDenoising Diffusion Probabilistic Models (DDPMs): DDPMs are a popular formulation of diffusion models introduced around 2020. They define a forward diffusion process that gradually adds Gaussian noise to data over discrete time steps and a learned reverse process that denoises step-by-step to generate samples. The reverse process is parameterized by a neural network predicting the mean and variance of the denoised data at each step. DDPMs use variational inference to train this reverse denoising process.\nScore-Based Generative Models (SGMs): SGMs view diffusion models through the lens of score matching. Instead of explicitly modeling the reverse conditional distributions as in DDPMs, SGMs learn the score function — the gradient of the log probability density of noisy data at different noise levels. The generation process then solves a reverse-time stochastic differential equation (SDE) guided by these learned scores. SGMs have shown high synthesis quality and are applied in various domains like image and speech synthesis.\nStochastic Differential Equations (SDEs): SDEs provide a continuous-time formulation of diffusion models. Instead of discrete noise steps, the forward noising process is modeled as an SDE, and the reverse generative process is another SDE that depends on the learned score function. This continuous approach generalizes DDPMs and SGMs, allowing infinitely many noise scales and principled sampling methods. The reverse SDE can be solved numerically to generate samples. This connection was formalized by Song, Kingma, et al. (2021), who showed that both DDPMs and SGMs can be seen as discretizations or special cases of these continuous-time SDE formulations"
  },
  {
    "objectID": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#flow-matching-models",
    "href": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#flow-matching-models",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Flow Matching models",
    "text": "Flow Matching models"
  },
  {
    "objectID": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#common-types-of-generative-ai-models",
    "href": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#common-types-of-generative-ai-models",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Common Types of Generative AI Models",
    "text": "Common Types of Generative AI Models\n\nGenerative Adversarial Networks (GANs)"
  },
  {
    "objectID": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#bibliography",
    "href": "blogs/ODE_Gen_AI/ODE_Gen_AI.html#bibliography",
    "title": "Introduction to differential equations and their application in generative AI",
    "section": "Bibliography",
    "text": "Bibliography\n\nContinuous Normalizing Flows. Vikram Voleti.\nICLR Blogpost by Ayan Das\nAn Intro to Diffusion Probabilistic Models by Ayan Das\nStochastic Differential Equations and Diffusion Models by Tanmaya Shekhar Dabral\nSong, Kingma 2021\nLangevin equation\nMIT Course of FM and Diffusion\n\nMaybe these:\n\nGeneral intro 1\nGeneral intro 2\n\nI think this is important:\n\nVAE Challenges:\n• Optimization can be difficult for large models\n• The ELBO enforces an information bottleneck (through its loss function) at the latent variables ‘z’, making VAE optimization prone to bad local minima.\n• Posterior collapse is a dreaded bad local minimum where the latents do not transmit any information Source: https://cs231n.stanford.edu/slides/2023/lecture_15.pdf"
  },
  {
    "objectID": "blogs.html#vae-and-cvae-derivation",
    "href": "blogs.html#vae-and-cvae-derivation",
    "title": "Blogs",
    "section": "",
    "text": "Published on: 2024-08-26\nThis is a brief preview of my first blog post. It discusses interesting topics such as Bayesian statistics, deep learning, and more.\nRead more"
  },
  {
    "objectID": "blogs.html#mathematical-derivation-of-vae-and-conditional-vae-cvae",
    "href": "blogs.html#mathematical-derivation-of-vae-and-conditional-vae-cvae",
    "title": "Blogs",
    "section": "Mathematical Derivation of VAE and Conditional VAE (cVAE)",
    "text": "Mathematical Derivation of VAE and Conditional VAE (cVAE)\nPublished on: Jan 2026\nA concise walkthrough of the mathematical foundations behind Variational Autoencoders (VAE) and their conditional extension (cVAE), highlighting key derivations and concepts for understanding how they work.\nRead more"
  },
  {
    "objectID": "blogs/VAE_and_cVAE_derivation/VAE_and_cVAE_derivation.html",
    "href": "blogs/VAE_and_cVAE_derivation/VAE_and_cVAE_derivation.html",
    "title": "Mathematical Derivation of VAE",
    "section": "",
    "text": "Mathematical Derivation of VAE\n\nWhat is the aim of blog?\nThis blog draws upon insights from this article and this post, with the objective of developing a deeper and more technical understanding of Variational Autoencoders (VAEs).\n\n\nIntroduction: What are Variational Autoencoders (VAEs)?\nVariational Autoencoders (VAEs) are a foundational class of latent variable models that combine probabilistic modeling with neural networks, enabling both representation learning and data generation. Unlike standard autoencoders, VAEs provide a principled probabilistic framework that allows us to reason about uncertainty, impose structure on latent spaces, and perform meaningful sampling.\nAt a high level, VAEs assume that high-dimensional observations are generated from a lower-dimensional latent variable through a stochastic process. Learning such models, however, presents a central challenge: the true posterior distribution over latent variables is intractable. Variational inference resolves this by introducing a tractable approximation and reframing learning as an optimization problem.\nThe goal of this post is to develop a precise and fully mathematical understanding of how VAEs are derived—from their generative assumptions to the Evidence Lower Bound (ELBO) that is optimized in practice. Rather than focusing on intuition alone, we walk through each step of the derivation, clarifying where approximations are introduced and why the final objective is both computable and effective.\n\n\nIntuition: What a VAE Is Really Learning\nA VAE assumes that each data point can be explained by a small set of hidden factors—latent variables—that capture the essential structure of the data.\nThe encoder does not map an input to a single point in latent space, but instead learns a distribution over plausible latent representations, reflecting uncertainty about how the data was generated.\nThe decoder then learns how to probabilistically reconstruct the data from samples drawn from this latent distribution.\nTraining a VAE is therefore a balancing act: we want latent representations that are expressive enough to reconstruct the data well, while also being regularized to follow a simple prior distribution so that the latent space remains smooth and generative.\n\n\nWhat are the assumptions of a VAE model?\nAssume we have a dataset \\(X\\):\n\\[X = [\\vec{x}^{(i)}]_{i=1}^N = \\{\\vec{x}^{(1)}, \\vec{x}^{(2)}, \\ldots, \\vec{x}^{(N)}\\}\\]\nEach \\(\\vec{x}^{(i)}\\) is IID. It can be continuous or discrete-valued.\nKey Generative Assumptions:\nThe VAE framework makes the following fundamental assumptions about how the observed data is generated:\n\n\n\n\n\n\nAssumption 1: Latent Prior Distribution\n\n\n\nEach latent vector \\(\\vec{z}^{(i)}\\) is drawn from a prior distribution:\n\\[\\vec{z}^{(i)} \\sim \\color{purple}{p_{\\theta^*}(\\vec{z})}\\]\nwhere \\(\\color{purple}{p_{\\theta^*}(\\vec{z})}\\) is the \\(\\color{purple}{\\text{prior}}\\) over the lower-dimensional latent space, parameterized by \\(\\color{purple}{\\theta^*}\\).\n\n\n\n\n\n\n\n\nAssumption 2: Conditional Likelihood\n\n\n\nEach observed data point is generated from its corresponding latent vector through a conditional distribution:\n\\[\\vec{x}^{(i)} \\sim \\color{blue}{p_{\\theta^*}(\\vec{x} \\mid \\vec{z} = \\vec{z}^{(i)})}\\]\nwhere \\(\\color{blue}{p_{\\theta^*}(\\vec{x} \\mid \\vec{z})}\\) is the model’s \\(\\color{blue}{\\text{likelihood}}\\) function, representing the probability of generating \\(\\vec{x}\\) given latent code \\(\\vec{z}\\).\n\n\nIn essence, we assume the observed high-dimensional dataset \\(X\\) is generated by a latent variable model with an underlying lower-dimensional random process \\(\\vec{z}\\).\nThe goal is to find the parameter \\(\\color{purple}{\\theta^*}\\) that makes our observed data as likely as possible under the model. In other words, we want to maximize the likelihood of the data:\n\\[\\color{purple}{\\theta^*} {\\color{black}{= \\arg \\max_{\\theta}\\prod_{i=1}^N}} \\color{teal}{p_\\theta(\\vec{x}^{(i)})}\\]\nHowever, it is usually more convenient and numerically stable to work with the log-likelihood (since logs turn products into sums). Therefore, we rewrite the objective as:\n\\[\\color{purple}{\\theta^*} \\color{black}{ = \\arg \\max_{\\theta}\\sum_{i=1}^N \\log} \\; \\color{teal}{p_\\theta(\\vec{x}^{(i)})}\\]\nTo compute \\(\\color{teal}{p_\\theta(\\vec{x}^{i})}\\), we marginalize over the latent variable \\(z\\):\n\\[\\color{teal}{p_\\theta(\\vec{x}^{i})} = \\int \\color{blue}{p_\\theta(x|z)}\\, \\color{orange}{p_\\theta(z)}\\, dz\\]\nHowever, directly computing this integral is typically intractable because it requires evaluating \\(p_\\theta(x|z)\\) for all possible values of \\(z\\). To make this computation practical, we introduce an auxiliary function, \\(\\color{green}{q_\\phi(z|x)}\\), called the variational distribution or approximate posterior. This function, parameterized by \\(\\color{green}{\\phi}\\), provides a tractable way to estimate which values of \\(z\\) are likely given a particular input \\(x\\).\n\n\nWhat is the role of VAE’s encoder and decoder?\nTo understand the VAE architecture, let’s start with Bayes’ theorem, which relates our key distributions:\n\\[\\color{red}{p_\\theta(z|x)} = \\frac{\\color{orange}{p_\\theta(z)} \\times \\color{blue}{p_\\theta(x|z)}}{\\color{teal}{p_\\theta(x)}}\\]\nThis equation shows that the posterior distribution \\(\\color{red}{p_\\theta(z|x)}\\) (the probability of latent code \\(z\\) given observation \\(x\\)) can be computed from the prior \\(\\color{orange}{p_\\theta(z)}\\), the likelihood \\(\\color{blue}{p_\\theta(x|z)}\\), and the evidence \\(\\color{teal}{p_\\theta(x)}\\).\n\nThe Challenge: Intractable Posterior\nThe posterior \\(\\color{red}{p_\\theta(z|x)}\\) is intractable to compute directly because it requires knowing \\(\\color{teal}{p_\\theta(x)}\\), which involves the difficult integral we discussed earlier. This is where the VAE’s two-part architecture comes in:\n\n\n\n\n\n\nThe Encoder (Recognition Network)\n\n\n\nThe encoder approximates the intractable posterior \\(\\color{red}{p_\\theta(z|x)}\\) using variational inference.\n\nWe introduce a variational distribution \\(\\color{green}{q_\\phi(z|x)}\\) that is designed to be tractable\nThe encoder learns parameters \\(\\color{green}{\\phi}\\) to make \\(\\color{green}{q_\\phi(z|x)} \\approx \\color{red}{p_\\theta(z|x)}\\) as close as possible\nRole: Given an input \\(x\\), the encoder outputs a distribution over likely latent codes \\(z\\)\n\n\n\n\n\n\n\n\n\nThe Decoder (Generative Network)\n\n\n\nThe decoder models the likelihood \\(\\color{blue}{p_\\theta(x|z)}\\), which is the generative part of the model.\n\nThe decoder learns parameters \\(\\color{blue}{\\theta}\\) to map from the latent space back to the data space\nRole: Given a latent code \\(z\\), the decoder outputs a distribution over possible reconstructions \\(x\\)\n\n\n\nIn summary: the encoder compresses observations \\(x\\) into latent representations \\(z\\), while the decoder reconstructs observations from latent codes.\n\n\n\nHow can we learn \\(\\color{green}{\\phi}\\) and \\(\\color{blue}{\\theta}\\) jointly?\nNow we have two sets of parameters to optimize:\n\nEncoder parameters \\(\\color{green}{\\phi}\\): control the approximate posterior \\(\\color{green}{q_\\phi(z|x)}\\)\nDecoder parameters \\(\\color{blue}{\\theta}\\): control the likelihood \\(\\color{blue}{p_\\theta(x|z)}\\) (and also appear in the true posterior \\(\\color{red}{p_\\theta(z|x)}\\))\n\nOur goal is to make the estimated posterior \\(\\color{green}{q_\\phi(z|x)}\\) as close as possible to the true (but intractable) posterior \\(\\color{red}{p_\\theta(z|x)}\\).\n\nMeasuring Closeness: The KL Divergence\nTo measure how “close” two probability distributions are, we use the Kullback-Leibler (KL) divergence. Specifically, we use the reverse KL divergence:\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\mathbb{E}_{z \\sim \\color{green}{q_\\phi(z|x)}} \\left[ \\log \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{red}{p_\\theta(z|x)}} \\right]\\]\nwhich can be written more explicitly as:\n\nFor discrete latent variables: \\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\sum_{z \\in Z} \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{red}{p_\\theta(z|x)}} \\right)\\]\nFor continuous latent variables: \\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{red}{p_\\theta(z|x)}} \\right) dz\\]\n\n\n\n\n\n\n\nKey Properties of KL Divergence\n\n\n\nUnderstanding these properties is crucial for grasping how VAE training works:\n1. Non-negativity: \\(\\text{KL}(q \\; || \\; p) \\geq 0\\) for any two distributions \\(q\\) and \\(p\\)\n\nThe KL divergence is always non-negative or zero\nThis property is fundamental and comes from Jensen’s inequality\nIntuitively: there’s always some “cost” to using an approximation unless it’s exact\n\n2. Zero if and only if distributions are identical: \\(\\text{KL}(q \\; || \\; p) = 0 \\iff q = p\\) (almost everywhere)\n\nWhen \\(q\\) and \\(p\\) are exactly the same, there’s no information loss\nThis is our ideal case: \\(\\color{green}{q_\\phi(z|x)} \\color{black}{=} \\color{red}{p_\\theta(z|x)}\\)\nIn practice, we get close but rarely achieve exactly zero\n\n3. Asymmetric (not a distance metric): \\(\\text{KL}(q \\; || \\; p) \\neq \\text{KL}(p \\; || \\; q)\\) in general\n\nThe order matters! \\(\\text{KL}(\\color{green}{q_\\phi} \\; || \\; \\color{red}{p_\\theta})\\) is called the reverse KL\nReverse KL encourages \\(q\\) to focus on regions where \\(p\\) has high probability\nThis is why VAE tends to produce “mode-covering” behavior rather than “mode-seeking”\n\n4. Measures information loss: It quantifies the expected extra bits (or nats) needed when using \\(q\\) to encode samples from \\(p\\)\n\nIn VAE context: how much information we lose by using \\(\\color{green}{q_\\phi(z|x)}\\) instead of the true \\(\\color{red}{p_\\theta(z|x)}\\)\n\n\n\nOur Training Objective:\nBy minimizing \\(\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right)\\), we make our encoder’s approximate posterior as accurate as possible. However, there’s a problem: we can’t directly compute this KL divergence because it involves the intractable posterior \\(\\color{red}{p_\\theta(z|x)}\\)!\nThis is where the ELBO (Evidence Lower Bound) comes in, which we’ll derive next.\n\n\n\nDeriving the Evidence Lower Bound (ELBO)\n\nThe Problem We Need to Solve\nRecall that we want to minimize \\(\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right)\\), but we can’t compute it directly because it involves the intractable posterior \\(\\color{red}{p_\\theta(z|x)}\\).\nThe variational inference idea is that we can derive an alternative objective that:\n\nIs tractable to compute\nStill allows us to optimize both \\(\\color{green}{\\phi}\\) and \\(\\color{blue}{\\theta}\\)\nAutomatically handles the intractability\n\nLet’s derive this alternative objective step by step.\n\n\nStep 1: Start with the KL Divergence\nFor continuous latent variables, the KL divergence is defined as:\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{red}{p_\\theta(z|x)}} \\right) dz\\]\n\n\nStep 2: Apply Bayes’ Rule to the Posterior\nUsing Bayes’ rule, we know that:\n\\[\\color{red}{p_\\theta(z|x)} = \\frac{p_\\theta(x,z)}{\\color{teal}{p_\\theta(x)}}\\]\nSubstituting this into our KL divergence:\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)} \\cdot \\color{teal}{p_\\theta(x)}}{p_\\theta(x,z)} \\right) dz\\]\n\n\nStep 3: Split the Logarithm\nUsing the property \\(\\log(ab/c) = \\log(a) + \\log(b) - \\log(c)\\):\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\int \\color{green}{q_\\phi(z|x)} \\left[ \\log(\\color{teal}{p_\\theta(x)}) + \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{p_\\theta(x,z)} \\right) \\right] dz\\]\n\n\nStep 4: Separate the Integral\nSplit into two integrals:\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\int \\color{green}{q_\\phi(z|x)} \\log(\\color{teal}{p_\\theta(x)}) \\, dz + \\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{p_\\theta(x,z)} \\right) dz\\]\n\n\n\n\n\n\nKey Observation: The Evidence is Constant\n\n\n\nThe first integral simplifies because \\(\\color{teal}{p_\\theta(x)}\\) doesn’t depend on \\(z\\):\n\\[\\int \\color{green}{q_\\phi(z|x)} \\log(\\color{teal}{p_\\theta(x)}) \\, dz = \\log(\\color{teal}{p_\\theta(x)}) \\int \\color{green}{q_\\phi(z|x)} \\, dz = \\log(\\color{teal}{p_\\theta(x)})\\]\nsince \\(\\int \\color{green}{q_\\phi(z|x)} \\, dz = 1\\) (probability distributions must integrate to 1).\n\n\nSo now we have:\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\log(\\color{teal}{p_\\theta(x)}) + \\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{p_\\theta(x,z)} \\right) dz\\]\n\n\nStep 5: Decompose the Joint Distribution\nThe joint distribution can be factored as:\n\\[p_\\theta(x,z) = \\color{blue}{p_\\theta(x|z)} \\cdot \\color{purple}{p_\\theta(z)}\\]\nSubstituting this:\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\log(\\color{teal}{p_\\theta(x)}) + \\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{blue}{p_\\theta(x|z)} \\cdot \\color{purple}{p_\\theta(z)}} \\right) dz\\]\n\n\nStep 6: Simplify Using Logarithm Properties\nUsing \\(\\log(a/(bc)) = \\log(a/b) - \\log(c)\\):\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\log(\\color{teal}{p_\\theta(x)}) + \\int \\color{green}{q_\\phi(z|x)} \\left[ \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{purple}{p_\\theta(z)}} \\right) - \\log(\\color{blue}{p_\\theta(x|z)}) \\right] dz\\]\n\n\nStep 7: Split Into Two Integrals\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\log(\\color{teal}{p_\\theta(x)}) + \\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{purple}{p_\\theta(z)}} \\right) dz - \\int \\color{green}{q_\\phi(z|x)} \\log(\\color{blue}{p_\\theta(x|z)}) \\, dz\\]\n\n\nStep 8: Recognize Standard Forms\nThe second term is the KL divergence between \\(\\color{green}{q_\\phi(z|x)}\\) and the prior \\(\\color{purple}{p_\\theta(z)}\\):\n\\[\\int \\color{green}{q_\\phi(z|x)} \\log \\left( \\frac{\\color{green}{q_\\phi(z|x)}}{\\color{purple}{p_\\theta(z)}} \\right) dz = \\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{purple}{p_\\theta(z)}\\right)\\]\nThe third term is an expectation under \\(\\color{green}{q_\\phi(z|x)}\\):\n\\[\\int \\color{green}{q_\\phi(z|x)} \\log(\\color{blue}{p_\\theta(x|z)}) \\, dz = \\mathbb{E}_{z \\sim \\color{green}{q_\\phi(z|x)}} \\left[ \\log(\\color{blue}{p_\\theta(x|z)}) \\right]\\]\nTherefore:\n\\[\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) = \\log(\\color{teal}{p_\\theta(x)}) + \\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{purple}{p_\\theta(z)}\\right) - \\mathbb{E}_{z \\sim \\color{green}{q_\\phi(z|x)}} \\left[ \\log(\\color{blue}{p_\\theta(x|z)}) \\right]\\]\n\n\nStep 9: Rearrange to Isolate the Evidence\nRearranging the equation:\n\\[\\log(\\color{teal}{p_\\theta(x)}) = \\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right) + \\underbrace{\\mathbb{E}_{z \\sim \\color{green}{q_\\phi(z|x)}} \\left[ \\log(\\color{blue}{p_\\theta(x|z)}) \\right] - \\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{purple}{p_\\theta(z)}\\right)}_{\\text{ELBO}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x)}\\]\n\n\n\n\n\n\nThe Evidence Lower Bound (ELBO)\n\n\n\nWe define the Evidence Lower Bound (ELBO) as:\n\\[\\text{ELBO}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x) = \\mathbb{E}_{z \\sim \\color{green}{q_\\phi(z|x)}} \\left[ \\log(\\color{blue}{p_\\theta(x|z)}) \\right] - \\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{purple}{p_\\theta(z)}\\right)\\]\nWhy is it called a “lower bound”?\nFrom the equation above: \\[\\log(\\color{teal}{p_\\theta(x)}) = \\underbrace{\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right)}_{\\geq 0} + \\text{ELBO}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x)\\]\nSince \\(\\text{KL} \\geq 0\\), we have: \\[\\text{ELBO}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x) \\leq \\log(\\color{teal}{p_\\theta(x)})\\]\nThe ELBO provides a lower bound on the log-evidence \\(\\log(\\color{teal}{p_\\theta(x)})\\)!\n\n\n\n\nStep 10: The VAE Loss Function\nIn practice, we want to minimize a loss function. The VAE loss is simply the negative ELBO:\n\\[\\mathcal{L}_{\\text{VAE}}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x) = -\\text{ELBO}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x)\\]\n\\[\\boxed{\\mathcal{L}_{\\text{VAE}}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x) = \\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{purple}{p_\\theta(z)}\\right) - \\mathbb{E}_{z \\sim \\color{green}{q_\\phi(z|x)}} \\left[ \\log(\\color{blue}{p_\\theta(x|z)}) \\right]}\\]\nWe seek the optimal parameters:\n\\[\\color{green}{\\phi}^*, \\color{blue}{\\theta}^* = \\arg\\min_{\\color{green}{\\phi}, \\color{blue}{\\theta}} \\mathcal{L}_{\\text{VAE}}(\\color{blue}{\\theta}, \\color{green}{\\phi}; x)\\]\n\n\n\n\n\n\nInterpreting the VAE Loss\n\n\n\nThe VAE loss has two terms:\n\nKL Regularization Term: \\(\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{purple}{p_\\theta(z)}\\right)\\)\n\nEncourages the encoder’s approximate posterior to stay close to the prior\nActs as a regularizer preventing overfitting\nEnsures the latent space has a nice structure\n\nReconstruction Term: \\(-\\mathbb{E}_{z \\sim \\color{green}{q_\\phi(z|x)}} \\left[ \\log(\\color{blue}{p_\\theta(x|z)}) \\right]\\)\n\nEncourages the decoder to reconstruct the input accurately\nWhen the decoder outputs Gaussian distributions, this becomes MSE (Mean Squared Error)\nWhen the decoder outputs Bernoulli distributions, this becomes BCE (Binary Cross-Entropy)\n\n\n\n\nParameter Names:\n\n\\(\\color{green}{\\phi}\\) are the variational parameters (encoder parameters)\n\\(\\color{blue}{\\theta}\\) are the generative parameters (decoder parameters)\n\nWhy This Solves Our Problem:\nMaximizing the ELBO (equivalently, minimizing \\(\\mathcal{L}_{\\text{VAE}}\\)) simultaneously:\n\nIncreases \\(\\log(\\color{teal}{p_\\theta(x)})\\) (generates more realistic samples)\nDecreases \\(\\text{KL}\\left(\\color{green}{q_\\phi(z|x)} \\; || \\; \\color{red}{p_\\theta(z|x)}\\right)\\) (better posterior approximation)\n\nAnd we can compute everything in the ELBO without ever needing the intractable \\(\\color{red}{p_\\theta(z|x)}\\)!\n\n\n\nKey Takeaways and Final Remarks\nThe Variational Autoencoder framework provides a principled solution to learning latent variable models when exact inference is intractable. By introducing a tractable variational posterior and optimizing the Evidence Lower Bound (ELBO), we transform an otherwise impossible likelihood maximization problem into a practical and scalable objective that can be optimized with stochastic gradient methods.\nFrom the derivation, several core insights emerge:\n\nFirst, the VAE objective naturally decomposes into two competing terms:\n\nA reconstruction term that encourages faithful data generation, via the decoder.\nA KL regularization term that shapes the latent space by keeping the approximate posterior close to the prior, via the encoder.\nThis trade-off is not an implementation detail, but a direct consequence of variational inference.\n\nSecond, maximizing the ELBO simultaneously improves data likelihood and posterior approximation, even though the true posterior is never computed explicitly.\n\nThis framework extends seamlessly to Conditional VAEs by conditioning both the encoder and decoder on auxiliary information. The mathematical structure of the ELBO remains unchanged, highlighting the flexibility and generality of variational inference as a modeling paradigm."
  }
]